#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import time
import random
import requests
import pymysql
import logging
import sys
import json
import re
import html
from urllib.parse import urlparse, parse_qs, unquote

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)

# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    'host': 'host.docker.internal',  # Dockerç¯å¢ƒä½¿ç”¨
    'user': 'root',
    'password': '12345abc',
    'db': 'weibo_comments',
    'charset': 'utf8mb4'
}

# å¾®åšCookie - å¿…é¡»æ›´æ–°ä¸ºå®é™…å€¼
WEIBO_COOKIE = 'WEIBOCN_FROM=1110006030; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WhODIMYYiX_1cT9hWlsy4uE5NHD95QN1h2pSo-EeK-4Ws4DqcjMi--NiK.Xi-2Ri--ciKnRi-zNS0npeKqfeo2f1Btt; SCF=Amn01UGa8qd90cxAtXIFyANa2mybBH-_EhpCcSC-7ll8MqNH7btWZA7FOOLY0TThlc62nSGEXhMdaUDCX3eWqMs.; SUB=_2A25FPHd5DeRhGeFG6lMX9SzNzjWIHXVmMPaxrDV6PUJbktAbLWfSkW1NfkUFVCMU-fFl8eZzQh8DQFA4Dr_YD3Ge; SSOLoginState=1748502313; ALF=1751094313; _T_WM=47059842252; MLOGIN=1; XSRF-TOKEN=8bfd69; M_WEIBOCN_PARAMS=luicode%3D20000061%26lfid%3D5135369449767367%26oid%3D5135369449767367%26fid%3D1076037879920498%26uicode%3D10000011'

# ç§»åŠ¨ç«¯APIå¤´
MOBILE_HEADERS = {
    "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1",
    "Cookie": WEIBO_COOKIE,
    "X-Requested-With": "XMLHttpRequest",
    "Referer": "https://m.weibo.cn/"
}

def clean_html_tags(text):
    """æ¸…é™¤HTMLæ ‡ç­¾"""
    if not text:
        return ""
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

def test_db_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    try:
        with pymysql.connect(**DB_CONFIG) as db:
            logging.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
    except pymysql.err.OperationalError as e:
        logging.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        logging.error("è¯·æ£€æŸ¥ä»¥ä¸‹é…ç½®:")
        logging.error(f"ä¸»æœº: {DB_CONFIG['host']}")
        logging.error(f"ç”¨æˆ·: {DB_CONFIG['user']}")
        logging.error(f"æ•°æ®åº“: {DB_CONFIG['db']}")
        logging.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
        return False
    except Exception as e:
        logging.error(f"âŒ æœªçŸ¥æ•°æ®åº“é”™è¯¯: {e}")
        return False

def get_weibo_url(index):
    """ä»æ•°æ®åº“è·å–å¾®åšURL"""
    try:
        with pymysql.connect(**DB_CONFIG,
            cursorclass=pymysql.cursors.DictCursor) as db:
            with db.cursor() as cursor:
                cursor.execute("SELECT url FROM weibo_urls WHERE id = %s", (index,))
                result = cursor.fetchone()
                
                if result:
                    logging.info(f"âœ… æˆåŠŸè·å–å¾®åš {index} çš„URL")
                    return result['url']
                else:
                    logging.warning(f"âš ï¸ æœªæ‰¾åˆ°å¾®åš {index} çš„URL")
                    logging.warning("è¯·ç¡®ä¿æ•°æ®åº“ä¸­å­˜åœ¨è¯¥IDçš„è®°å½•:")
                    logging.warning(f"ç¤ºä¾‹SQL: INSERT INTO weibo_urls (id, url) VALUES ({index}, 'ä½ çš„å¾®åšURL');")
                    return None
    except pymysql.err.ProgrammingError as e:
        if "doesn't exist" in str(e):
            logging.error(f"âŒ è¡¨ä¸å­˜åœ¨é”™è¯¯: {e}")
            logging.error("è¯·åˆ›å»ºè¡¨: CREATE TABLE weibo_urls (id INT PRIMARY KEY, url VARCHAR(500) NOT NULL);")
        else:
            logging.error(f"âŒ SQLé”™è¯¯: {e}")
        return None
    except Exception as e:
        logging.error(f"âŒ è·å–URLå¤±è´¥: {e}")
        return None

def create_comments_table(index):
    """åˆ›å»ºè¯„è®ºå­˜å‚¨è¡¨ï¼ˆç¡®ä¿è¡¨ç»“æ„æ­£ç¡®ï¼‰"""
    try:
        with pymysql.connect(**DB_CONFIG) as db:
            with db.cursor() as cursor:
                table_name = f"comments_{index}"
                
                # å…ˆåˆ é™¤æ—§è¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ä»¥ç¡®ä¿æ–°ç»“æ„
                cursor.execute(f"DROP TABLE IF EXISTS `{table_name}`")
                
                # åˆ›å»ºæ–°è¡¨ï¼ˆåŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—ï¼‰
                cursor.execute(f"""
                    CREATE TABLE `{table_name}` (
                        id INT AUTO_INCREMENT PRIMARY KEY,
                        user_id VARCHAR(50),
                        username VARCHAR(100),  # ç¡®ä¿åˆ—åæ­£ç¡®
                        comment TEXT,
                        like_count INT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
                """)
                logging.info(f"âœ… è¡¨ {table_name} åˆ›å»ºæˆåŠŸï¼ˆæ–°ç»“æ„ï¼‰")
                return True
    except Exception as e:
        logging.error(f"âŒ åˆ›å»ºè¡¨å¤±è´¥: {e}")
        return False

def save_comment(index, user_id, username, comment, like_count):
    """ä¿å­˜è¯„è®ºåˆ°æ•°æ®åº“"""
    try:
        with pymysql.connect(**DB_CONFIG) as db:
            with db.cursor() as cursor:
                table_name = f"comments_{index}"
                cursor.execute(f"""
                    INSERT INTO `{table_name}` (user_id, username, comment, like_count)
                    VALUES (%s, %s, %s, %s)
                """, (user_id, username, comment, like_count))
                db.commit()
                logging.debug(f"ğŸ’¾ ä¿å­˜è¯„è®º: {comment[:30]}...")
                return True
    except Exception as e:
        logging.error(f"âŒ ä¿å­˜è¯„è®ºå¤±è´¥: {e}")
        return False

def extract_weibo_id(url):
    """ä»å¾®åšURLä¸­æå–å¾®åšID"""
    try:
        # è§£ç URLä»¥é˜²ç¼–ç é—®é¢˜
        decoded_url = unquote(url)
        
        # å°è¯•ä»URLä¸­ç›´æ¥è·å–ID
        parsed = urlparse(decoded_url)
        path_segments = parsed.path.strip('/').split('/')
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç§»åŠ¨ç«¯URLæ ¼å¼ï¼š/detail/{weibo_id}
        if 'detail' in path_segments:
            weibo_id = path_segments[-1]
            if weibo_id.isdigit():
                return weibo_id
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç§»åŠ¨ç«¯URLæ ¼å¼ï¼š/status/{weibo_id}
        if 'status' in path_segments:
            weibo_id = path_segments[-1]
            if weibo_id.isdigit():
                return weibo_id
        
        # å°è¯•ä»æŸ¥è¯¢å‚æ•°ä¸­è·å–ID
        query_params = parse_qs(parsed.query)
        
        # ä¼˜å…ˆå°è¯•è·å–midå‚æ•°
        if 'mid' in query_params:
            weibo_id = query_params['mid'][0]
            if weibo_id.isdigit():
                return weibo_id
        
        # å°è¯•è·å–idå‚æ•°
        if 'id' in query_params:
            weibo_id = query_params['id'][0]
            if weibo_id.isdigit():
                return weibo_id
        
        # å°è¯•ä»URLæœ«å°¾æå–æ•°å­—ID
        match = re.search(r'(\d{10,})$', parsed.path)
        if match:
            return match.group(1)
        
        # å°è¯•ä»URLä¸­æå–çŸ­IDï¼ˆbase62æ ¼å¼ï¼‰
        match = re.search(r'[^a-zA-Z0-9]([a-zA-Z0-9]{9})$', parsed.path)
        if match:
            base62_id = match.group(1)
            logging.info(f"ğŸ” æ£€æµ‹åˆ°çŸ­IDæ ¼å¼: {base62_id}")
            return get_mid_from_short_id(base62_id)
        
        logging.error(f"âŒ æ— æ³•ä»URLæå–å¾®åšID: {url}")
        return None
    except Exception as e:
        logging.error(f"âŒ æå–å¾®åšIDå‡ºé”™: {e}")
        return None

def get_mid_from_short_id(short_id):
    """å°†çŸ­ID(base62)è½¬æ¢ä¸ºæ•°å­—ID(mid)"""
    try:
        url = f"https://m.weibo.cn/status/{short_id}"
        logging.info(f"ğŸŒ è¯·æ±‚çŸ­IDè½¬æ¢URL: {url}")
        
        response = requests.get(url, headers=MOBILE_HEADERS, timeout=15)
        response.raise_for_status()
        
        # ä»å“åº”ä¸­æå–æ•°å­—ID
        match = re.search(r'"id":\s*"(\d{16,})"', response.text)
        if match:
            mid = match.group(1)
            logging.info(f"âœ… æˆåŠŸè½¬æ¢çŸ­ID: {short_id} -> {mid}")
            return mid
        
        # å°è¯•å¦ä¸€ç§æ¨¡å¼
        match = re.search(r'weibo\.com/\d+/(\d{16,})', response.text)
        if match:
            mid = match.group(1)
            logging.info(f"âœ… æˆåŠŸæå–æ•°å­—ID: {mid}")
            return mid
        
        logging.error("âŒ æ— æ³•ä»é¡µé¢æå–æ•°å­—ID")
        return None
    except Exception as e:
        logging.error(f"âŒ çŸ­IDè½¬æ¢å¤±è´¥: {e}")
        return None

def get_comments_from_api(weibo_id, max_id=None):
    """ä»ç§»åŠ¨ç«¯APIè·å–è¯„è®ºæ•°æ®"""
    if max_id and max_id != "0":
        url = f"https://m.weibo.cn/comments/hotflow?id={weibo_id}&mid={weibo_id}&max_id={max_id}&max_id_type=0"
    else:
        url = f"https://m.weibo.cn/comments/hotflow?id={weibo_id}&mid={weibo_id}&max_id_type=0"
    
    logging.info(f"ğŸŒ è¯·æ±‚API: {url}")
    
    try:
        response = requests.get(url, headers=MOBILE_HEADERS, timeout=15)
        response.raise_for_status()
        
        # æ£€æŸ¥å“åº”å†…å®¹
        if response.text.strip() == "":
            logging.error("âŒ APIè¿”å›ç©ºå“åº”")
            return None
        
        return response.json()
    except requests.exceptions.RequestException as e:
        logging.error(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}")
        return None
    except json.JSONDecodeError:
        logging.error(f"âŒ JSONè§£æå¤±è´¥ï¼ŒåŸå§‹å“åº”: {response.text[:200]}...")
        return None

def crawl_comments(url, index):
    """çˆ¬å–å¾®åšè¯„è®ºï¼ˆä½¿ç”¨ç§»åŠ¨ç«¯APIï¼‰"""
    if not url:
        logging.error("âŒ URLä¸ºç©ºï¼Œæ— æ³•çˆ¬å–")
        return 0
    
    logging.info(f"ğŸŒ å¼€å§‹çˆ¬å–å¾®åš {index}: {url}")
    
    # ä»URLä¸­æå–å¾®åšID
    weibo_id = extract_weibo_id(url)
    if not weibo_id:
        # å°è¯•ä»URLæœ«å°¾ç›´æ¥æå–æ•°å­—ID
        match = re.search(r'(\d{16,})$', url)
        if match:
            weibo_id = match.group(1)
            logging.info(f"âœ… ä»URLæœ«å°¾æå–åˆ°å¾®åšID: {weibo_id}")
        else:
            logging.error("âŒ æ— æ³•è·å–å¾®åšIDï¼Œåœæ­¢çˆ¬å–")
            return 0
    
    logging.info(f"ğŸ” ä½¿ç”¨å¾®åšID: {weibo_id}")
    
    comments_data = []
    max_id = None
    page = 1
    total_comments = 0
    max_retries = 3
    
    while True:
        logging.info(f"ğŸ“– æ­£åœ¨è·å–ç¬¬ {page} é¡µè¯„è®º...")
        
        retry_count = 0
        data = None
        
        while retry_count < max_retries:
            data = get_comments_from_api(weibo_id, max_id)
            if data:
                break
            retry_count += 1
            logging.warning(f"âš ï¸ ç¬¬ {retry_count} æ¬¡é‡è¯•...")
            time.sleep(random.uniform(3, 8))
        
        if not data:
            logging.error("âŒ å¤šæ¬¡å°è¯•åä»æ— æ³•è·å–æ•°æ®ï¼Œç»ˆæ­¢æŠ“å–")
            break
        
        # æ£€æŸ¥APIè¿”å›çŠ¶æ€
        if data.get("ok") != 1:
            error_msg = data.get("msg", "æœªçŸ¥é”™è¯¯")
            logging.error(f"âŒ APIè¿”å›é”™è¯¯: {error_msg}")
            
            # å¦‚æœæ˜¯è¯·æ±‚å¤ªé¢‘ç¹çš„é”™è¯¯ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
            if "è¯·æ±‚è¿‡äºé¢‘ç¹" in error_msg or "max_id" in error_msg:
                wait_time = random.uniform(15, 30)
                logging.warning(f"â³ æ£€æµ‹åˆ°è¯·æ±‚é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
                time.sleep(wait_time)
                continue
            else:
                break
        
        # è§£æè¯„è®ºæ•°æ®
        comment_list = data.get("data", {}).get("data", [])
        if not comment_list:
            logging.info("âœ… æ²¡æœ‰æ›´å¤šè¯„è®º")
            break
        
        logging.info(f"âœ… æœ¬é¡µè·å–åˆ° {len(comment_list)} æ¡è¯„è®º")
        
        for comment in comment_list:
            try:
                # å¤„ç†ç‰¹æ®Šå­—ç¬¦å’ŒHTMLå®ä½“
                content = comment.get("text", "")
                content = html.unescape(content)
                content = clean_html_tags(content)
                
                # ä¿å­˜è¯„è®ºåˆ°æ•°æ®åº“
                if save_comment(
                    index,
                    str(comment["user"]["id"]),  # ç¡®ä¿user_idæ˜¯å­—ç¬¦ä¸²
                    comment["user"]["screen_name"],
                    content,
                    comment.get("like_count", 0)
                ):
                    total_comments += 1
            except KeyError as e:
                logging.warning(f"âš ï¸ è§£æè¯„è®ºæ—¶ç¼ºå°‘å…³é”®å­—æ®µ: {e}")
            except Exception as e:
                logging.error(f"âŒ ä¿å­˜è¯„è®ºæ—¶å‡ºé”™: {e}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
        max_id = data.get("data", {}).get("max_id")
        if not max_id or max_id == "0" or max_id == 0:
            logging.info("âœ… å·²åˆ°æœ€åä¸€é¡µ")
            break
        
        page += 1
        # éšæœºå»¶è¿Ÿé˜²æ­¢è¢«å°
        delay = random.uniform(8, 20)
        logging.info(f"â³ ç­‰å¾… {delay:.2f} ç§’åç»§ç»­...")
        time.sleep(delay)
    
    logging.info(f"ğŸ‰ å¾®åš {index} çˆ¬å–å®Œæˆ! å…±è·å– {total_comments} æ¡è¯„è®º")
    return total_comments

def main():
    logging.info("\n" + "="*60)
    logging.info("å¾®åšè¯„è®ºçˆ¬è™«å¯åŠ¨ï¼ˆç§»åŠ¨ç«¯APIç‰ˆï¼‰")
    logging.info("="*60)
    
    # éªŒè¯Cookie
    if WEIBO_COOKIE == 'YOUR_ACTUAL_WEIBO_COOKIE':
        logging.error("âŒ è¯·æ›´æ–°WEIBO_COOKIEé…ç½®")
        logging.error("ä»æµè§ˆå™¨è·å–æœ‰æ•ˆçš„å¾®åšCookie")
        return
    
    # æµ‹è¯•æ•°æ®åº“è¿æ¥
    if not test_db_connection():
        return
    
    # éœ€è¦çˆ¬å–çš„å¾®åšIDåˆ—è¡¨
    weibo_ids = [1, 2, 3, 4, 5,]
    total_all_comments = 0
    
    for index in weibo_ids:
        logging.info("\n" + "="*60)
        logging.info(f"å¼€å§‹å¤„ç†å¾®åš {index}")
        logging.info("="*60)
        
        url = get_weibo_url(index)
        if not url:
            continue
            
        if create_comments_table(index):
            comments_count = crawl_comments(url, index)
            total_all_comments += comments_count
        else:
            logging.error(f"âŒ æ— æ³•ä¸ºå¾®åš {index} åˆ›å»ºè¡¨")
    
    logging.info("\n" + "="*60)
    logging.info(f"æ‰€æœ‰å¾®åšå¤„ç†å®Œæˆ! å…±çˆ¬å– {total_all_comments} æ¡è¯„è®º")
    logging.info("="*60)

if __name__ == '__main__':
    main()
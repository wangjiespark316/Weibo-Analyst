#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®åšè¯„è®ºåˆ†è¯å¤„ç†
"""
import jieba
import pymysql
import logging
import os

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# æ•°æ®åº“é…ç½®ï¼ˆä¸çˆ¬è™«ä¸€è‡´ï¼‰
DB_CONFIG = {
    'host': 'host.docker.internal',
    'user': 'root',
    'password': '12345abc',
    'db': 'weibo_comments',
    'charset': 'utf8mb4',
    'cursorclass': pymysql.cursors.DictCursor
}

# åŠ è½½è¯å…¸å’Œåœç”¨è¯
def load_resources():
    """åŠ è½½åˆ†è¯è¯å…¸å’Œåœç”¨è¯"""
    try:
        jieba.load_userdict("SogouLabDic.txt")
        jieba.load_userdict("dict_baidu_utf8.txt")
        jieba.load_userdict("dict_pangu.txt")
        jieba.load_userdict("dict_sougou_utf8.txt")
        jieba.load_userdict("dict_tencent_utf8.txt")
        jieba.load_userdict("my_dict.txt")
        logging.info("âœ… è‡ªå®šä¹‰è¯å…¸åŠ è½½æˆåŠŸ")
        
        with open('Stopword.txt', 'r', encoding='utf-8') as f:
            stopwords = {line.strip() for line in f}
        logging.info(f"âœ… åœç”¨è¯è¡¨åŠ è½½æˆåŠŸï¼Œå…± {len(stopwords)} ä¸ªåœç”¨è¯")
        return stopwords
    except Exception as e:
        logging.error(f"âŒ èµ„æºåŠ è½½å¤±è´¥: {e}")
        return None

def process_comments(weibo_ids, stopwords):
    """å¤„ç†æŒ‡å®šå¾®åšçš„è¯„è®º"""
    try:
        # åˆ›å»ºå­˜å‚¨ç›®å½•
        os.makedirs("cut_data", exist_ok=True)
        
        with pymysql.connect(**DB_CONFIG) as db:
            for index in weibo_ids:
                table_name = f"comments_{index}"
                output_file = f"cut_data/data_full_{index}.dat"
                
                logging.info(f"ğŸ“Š å¼€å§‹å¤„ç†å¾®åš {index} çš„è¯„è®º...")
                
                # è·å–è¯„è®ºæ•°æ®
                with db.cursor() as cursor:
                    cursor.execute(f"SELECT comment FROM `{table_name}`")
                    comments = cursor.fetchall()
                
                if not comments:
                    logging.warning(f"âš ï¸ å¾®åš {index} æ²¡æœ‰è¯„è®ºæ•°æ®")
                    continue
                
                # å¤„ç†å¹¶ä¿å­˜åˆ†è¯ç»“æœ
                with open(output_file, "w", encoding='utf-8') as fo:
                    for i, comment in enumerate(comments):
                        text = comment['comment']
                        seg_list = jieba.cut(text)
                        
                        # è¿‡æ»¤åœç”¨è¯å¹¶å†™å…¥æ–‡ä»¶
                        filtered_words = [word for word in seg_list if word.strip() and word not in stopwords]
                        fo.write(' '.join(filtered_words) + '\n')
                        
                        if (i + 1) % 100 == 0:
                            logging.info(f"âœ‚ï¸ å·²å¤„ç† {i+1}/{len(comments)} æ¡è¯„è®º")
                
                logging.info(f"âœ… å¾®åš {index} åˆ†è¯å®Œæˆ! å…±å¤„ç† {len(comments)} æ¡è¯„è®º")
                logging.info(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {output_file}")
                
        return True
    except pymysql.err.ProgrammingError as e:
        if "doesn't exist" in str(e):
            logging.error(f"âŒ è¡¨ä¸å­˜åœ¨: {table_name}")
            logging.error("è¯·å…ˆè¿è¡Œçˆ¬è™«ç¨‹åºæ”¶é›†è¯„è®ºæ•°æ®")
        else:
            logging.error(f"âŒ æ•°æ®åº“é”™è¯¯: {e}")
        return False
    except Exception as e:
        logging.error(f"âŒ å¤„ç†å¤±è´¥: {e}")
        return False

if __name__ == '__main__':
    # éœ€è¦å¤„ç†çš„å¾®åšIDåˆ—è¡¨ï¼ˆä¸çˆ¬è™«ä¸€è‡´ï¼‰
    weibo_ids = [101, 102, 103, 104, 105]
    
    logging.info("\n" + "="*60)
    logging.info("å¾®åšè¯„è®ºåˆ†è¯å¤„ç†å¯åŠ¨")
    logging.info("="*60)
    
    # åŠ è½½è¯å…¸èµ„æº
    stopwords = load_resources()
    if not stopwords:
        exit(1)
    
    # å¤„ç†è¯„è®º
    if process_comments(weibo_ids, stopwords):
        logging.info("\nğŸ‰ æ‰€æœ‰å¾®åšè¯„è®ºåˆ†è¯å¤„ç†å®Œæˆ!")
    else:
        logging.error("\nâŒ å¤„ç†è¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯")